{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Resia05/unsupervised_learning/blob/main/%D0%9A%D0%BE%D0%BF%D1%96%D1%8F_%D0%B7%D0%B0%D0%BF%D0%B8%D1%81%D0%BD%D0%B8%D0%BA%D0%B0_%22%D0%92%D1%81%D1%82%D1%83%D0%BF_%D0%B4%D0%BE_%D0%BD%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D0%B8%D1%85_%D0%BC%D0%B5%D1%80%D0%B5%D0%B6_ipynb%22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Секція 1. Логістична регресія з нуля.**\n",
        "\n",
        "Будемо крок за кроком будувати модель лог регресії з нуля для передбачення, чи буде врожай більше за 80 яблук (задача подібна до лекційної, але на класифікацію).\n",
        "\n",
        "Давайте нагадаємо основні формули для логістичної регресії.\n",
        "\n",
        "### Функція гіпотези - обчислення передбачення у логістичній регресії:\n",
        "\n",
        "$$\n",
        "\\hat{y} = \\sigma(x W^T + b) = \\frac{1}{1 + e^{-(x W^T + b)}}\n",
        "$$\n",
        "\n",
        "Де:\n",
        "- $ \\hat{y} $ — це ймовірність \"позитивного\" класу.\n",
        "- $ x $ — це вектор (або матриця для набору прикладів) вхідних даних.\n",
        "- $ W $ — це вектор (або матриця) вагових коефіцієнтів моделі.\n",
        "- $ b $ — це зміщення (bias).\n",
        "- $ \\sigma(z) $ — це сигмоїдна функція активації.\n",
        "\n",
        "### Як обчислюється сигмоїдна функція:\n",
        "\n",
        "Сигмоїдна функція $ \\sigma(z) $ має вигляд:\n",
        "\n",
        "$$\n",
        "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "$$\n",
        "\n",
        "Ця функція перетворює будь-яке дійсне значення $ z $ в інтервал від 0 до 1, що дозволяє інтерпретувати вихід як ймовірність для логістичної регресії.\n",
        "\n",
        "### Формула функції втрат для логістичної регресії (бінарна крос-ентропія):\n",
        "\n",
        "Функція втрат крос-ентропії оцінює, наскільки добре модель передбачає класи, порівнюючи передбачені ймовірності $ \\hat{y} $ із справжніми мітками $ y $. Формула наступна:\n",
        "\n",
        "$$\n",
        "L(y, \\hat{y}) = - \\left[ y \\cdot \\log(\\hat{y}) + (1 - y) \\cdot \\log(1 - \\hat{y}) \\right]\n",
        "$$\n",
        "\n",
        "Де:\n",
        "- $ y $ — це справжнє значення (мітка класу, 0 або 1).\n",
        "- $ \\hat{y} $ — це передбачене значення (ймовірність).\n",
        "\n"
      ],
      "metadata": {
        "id": "lbLHTNfSclli"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.\n",
        "Тут вже наведений код для ініціювання набору даних в форматі numpy. Перетворіть `inputs`, `targets` на `torch` тензори. Виведіть результат на екран."
      ],
      "metadata": {
        "id": "GtOYB-RHfc_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "3BNXSR-VdYKQ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QLKZ77x4v_-v"
      },
      "outputs": [],
      "source": [
        "# Вхідні дані (temp, rainfall, humidity)\n",
        "inputs = np.array([[73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70]], dtype='float32')\n",
        "\n",
        "# Таргети (apples > 80)\n",
        "targets = np.array([[0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1]], dtype='float32')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Перетворення на torch тензори\n",
        "inputs_tensor = torch.from_numpy(inputs)\n",
        "targets_tensor = torch.from_numpy(targets)\n",
        "\n",
        "# Виведення результату\n",
        "inputs_tensor, targets_tensor"
      ],
      "metadata": {
        "id": "KjoeaDrk6fO7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12d70fc5-0949-4346-9e36-5b4b0ba00313"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 73.,  67.,  43.],\n",
              "         [ 91.,  88.,  64.],\n",
              "         [ 87., 134.,  58.],\n",
              "         [102.,  43.,  37.],\n",
              "         [ 69.,  96.,  70.]]),\n",
              " tensor([[0.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [0.],\n",
              "         [1.]]))"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Ініціюйте ваги `w`, `b` для моделі логістичної регресії потрібної форми зважаючи на розмірності даних випадковими значеннями з нормального розподілу. Лишаю тут код для фіксації `random_seed`."
      ],
      "metadata": {
        "id": "iKzbJKfOgGV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.random.manual_seed(1)"
      ],
      "metadata": {
        "id": "aXhKw6Tdj1-d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8fa203a-ed6a-4d17-ece5-343f112179a2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7cc7502b8cd0>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ініціалізація ваг w та зміщення b з requires_grad=True\n",
        "w = torch.randn(inputs.shape[1], 1, requires_grad=True)  # Вектор ваг (3x1)\n",
        "b = torch.randn(1, requires_grad=True)  # Зсув (скаляр)\n",
        "\n",
        "# Виведення ініційованих значень\n",
        "w, b"
      ],
      "metadata": {
        "id": "eApcB7eb6h9o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d96e88b8-f31f-4701-e06e-8c3977ea878c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[-1.0276],\n",
              "         [-0.5631],\n",
              "         [-0.8923]], requires_grad=True),\n",
              " tensor([-0.0583], requires_grad=True))"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Напишіть функцію `model`, яка буде обчислювати функцію гіпотези в логістичній регресії і дозволяти робити передбачення на основі введеного рядка даних і коефіцієнтів в змінних `w`, `b`.\n",
        "\n",
        "  **Важливий момент**, що функція `model` робить обчислення на `torch.tensors`, тож для математичних обчислень використовуємо фукнціонал `torch`, наприклад:\n",
        "  - обчсилення $e^x$: `torch.exp(x)`\n",
        "  - обчсилення $log(x)$: `torch.log(x)`\n",
        "  - обчислення середнього значення вектору `x`: `torch.mean(x)`\n",
        "\n",
        "  Використайте функцію `model` для обчислення передбачень з поточними значеннями `w`, `b`.Виведіть результат обчислень на екран.\n",
        "\n",
        "  Проаналізуйте передбачення. Чи не викликають вони у вас підозр? І якщо викликають, то чим це може бути зумовлено?"
      ],
      "metadata": {
        "id": "nYGxNGTaf5s6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Сигмоїдна функція\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + torch.exp(-x))\n",
        "\n",
        "# Функція моделі логістичної регресії\n",
        "def model(x, w, b):\n",
        "    z = torch.matmul(x, w) + b  # Обчислення лінійної комбінації вхідних даних та ваг\n",
        "    y_hat = sigmoid(z)  # Застосування сигмоїдної функції\n",
        "    return y_hat\n",
        "\n",
        "# Приклад обчислення передбачень\n",
        "predictions = model(inputs_tensor, w, b)\n",
        "\n",
        "# Виведення передбачень\n",
        "predictions"
      ],
      "metadata": {
        "id": "pSz2j4Fh6jBv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e1d866b-362a-4010-ed82-29cca16affe2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.]], grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Якщо значення ваг\n",
        "𝑊\n",
        "та зміщення\n",
        "𝑏\n",
        "недостатньо великі (особливо якщо\n",
        "𝑏\n",
        "є від'ємним), значення\n",
        "𝑧\n",
        "можуть вийти від'ємними або близькими до нуля.  Якщо\n",
        "𝑧\n",
        "є від'ємним, ми отримаємо передбачення, що наближаються до 0"
      ],
      "metadata": {
        "id": "Sur3S4F_-eI4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Напишіть функцію `binary_cross_entropy`, яка приймає на вхід передбачення моделі `predicted_probs` та справжні мітки в даних `true_labels` і обчислює значення втрат (loss)  за формулою бінарної крос-ентропії для кожного екземпляра та вертає середні втрати по всьому набору даних.\n",
        "  Використайте функцію `binary_cross_entropy` для обчислення втрат для поточних передбачень моделі."
      ],
      "metadata": {
        "id": "O2AGM0Mb2yHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def binary_cross_entropy(predicted_probs, true_labels):\n",
        "    # Обчислення крос-ентропії для кожного екземпляра\n",
        "    loss = - (true_labels * torch.log(predicted_probs) + (1 - true_labels) * torch.log(1 - predicted_probs))\n",
        "    # Обчислення середнього значення втрат\n",
        "    mean_loss = torch.mean(loss)\n",
        "    return mean_loss\n",
        "\n",
        "# Використовуємо поточні передбачення для обчислення втрат\n",
        "loss_value = binary_cross_entropy(predictions, targets_tensor)\n",
        "\n",
        "# Виведення втрат\n",
        "loss_value\n"
      ],
      "metadata": {
        "id": "1bWlovvx6kZS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc071dc3-75b3-45ae-95d0-95377ff1fc84"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(nan, grad_fn=<MeanBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Зробіть зворотнє поширення помилки і виведіть градієнти за параметрами `w`, `b`. Проаналізуйте їх значення. Як гадаєте, чому вони саме такі?"
      ],
      "metadata": {
        "id": "ZFKpQxdHi1__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Обчислення передбачень\n",
        "predictions = model(inputs_tensor, w, b)\n",
        "\n",
        "# Обчислення втрат\n",
        "loss = binary_cross_entropy(predictions, targets_tensor)\n",
        "\n",
        "# Зворотнє поширення помилки\n",
        "loss.backward()\n",
        "\n",
        "# Виведення градієнтів для ваг та зміщення\n",
        "print(f'Градієнти для ваг (w): {w.grad}')\n",
        "print(f'Градієнти для зміщення (b): {b.grad}')\n"
      ],
      "metadata": {
        "id": "YAbXUNSJ6mCl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c28b0ef-0b2e-419f-cf1d-9fc6ec1d3e21"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Градієнти для ваг (w): tensor([[nan],\n",
            "        [nan],\n",
            "        [nan]])\n",
            "Градієнти для зміщення (b): tensor([nan])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "nan ймовірно виникає через 0 у прогнозах. При обчисленнях це призводить до помилок."
      ],
      "metadata": {
        "id": "L5cFlQhACUDb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Що сталось?**\n",
        "\n",
        "В цій задачі, коли ми ініціювали значення випадковими значеннями з нормального розподілу, насправді ці значення не були дуже гарними стартовими значеннями і привели до того, що градієнти стали дуже малими або навіть рівними нулю (це призводить до того, що градієнти \"зникають\"), і відповідно при оновленні ваг у нас не буде нічого змінюватись. Це називається `gradient vanishing`. Це відбувається через **насичення сигмоїдної функції активації.**\n",
        "\n",
        "У нашій задачі ми використовуємо сигмоїдну функцію активації, яка має такий вигляд:\n",
        "\n",
        "   $$\n",
        "   \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "   $$\n",
        "\n",
        "\n",
        "Коли значення $z$ дуже велике або дуже мале, сигмоїдна функція починає \"насичуватись\". Це означає, що для великих позитивних $z$ сигмоїда наближається до 1, а для великих негативних — до 0. В цих діапазонах градієнти починають стрімко зменшуватись і наближаються до нуля (бо градієнт - це похідна, похідна на проміжку функції, де вона паралельна осі ОХ, дорівнює 0), що робить оновлення ваг неможливим.\n",
        "\n",
        "![](https://editor.analyticsvidhya.com/uploads/27889vaegp.png)\n",
        "\n",
        "У логістичній регресії $ z = x \\cdot w + b $. Якщо ваги $w, b$ - великі, значення $z$ також буде великим, і сигмоїда перейде в насичену область, де градієнти дуже малі.\n",
        "\n",
        "Саме це сталося в нашій задачі, де великі випадкові значення ваг викликали насичення сигмоїдної функції. Це в свою чергу призводить до того, що під час зворотного поширення помилки (backpropagation) модель оновлює ваги дуже повільно або зовсім не оновлює. Це називається проблемою **зникнення градієнтів** (gradient vanishing problem).\n",
        "\n",
        "**Що ж робити?**\n",
        "Ініціювати ваги маленькими значеннями навколо нуля. Наприклад ми можемо просто в існуючій ініціалізації ваги розділити на 1000. Можна також використати інший спосіб ініціалізації вагів - інформація про це [тут](https://www.geeksforgeeks.org/initialize-weights-in-pytorch/).\n",
        "\n",
        "Як це робити - показую нижче. **Виконайте код та знову обчисліть передбачення, лосс і виведіть градієнти.**\n",
        "\n",
        "А я пишу пояснення, чому просто не зробити\n",
        "\n",
        "```\n",
        "w = torch.randn(1, 3, requires_grad=True)/1000\n",
        "b = torch.randn(1, requires_grad=True)/1000\n",
        "```\n",
        "\n",
        "Нам потрібно, аби тензори вагів були листовими (leaf tensors).\n",
        "\n",
        "1. **Що таке листовий тензор**\n",
        "Листовий тензор — це тензор, який був створений користувачем безпосередньо і з якого починається обчислювальний граф. Якщо такий тензор має `requires_grad=True`, PyTorch буде відслідковувати всі операції, виконані над ним, щоб правильно обчислювати градієнти під час навчання.\n",
        "\n",
        "2. **Чому ми використовуємо `w.data` замість звичайних операцій**\n",
        "Якщо ми просто виконали б операції, такі як `(w - 0.5) / 100`, ми б отримали **новий тензор**, який вже не був би листовим тензором, оскільки ці операції створюють **новий** тензор, а не модифікують існуючий.\n",
        "\n",
        "  Проте, щоб залишити наші тензори ваги `w` та зміщення `b` листовими і продовжити можливість відстеження градієнтів під час тренування, ми використовуємо атрибут `.data`. Цей атрибут дозволяє **виконувати операції in-place (прямо на існуючому тензорі)** без зміни самого об'єкта тензора. Отже, тензор залишається листовим, і PyTorch може коректно обчислювати його градієнти.\n",
        "\n",
        "3. **Чому важливо залишити тензор листовим**\n",
        "Якщо тензор більше не є листовим (наприклад, через проведення операцій, що створюють нові тензори), ви не зможете отримати градієнти за допомогою `w.grad` чи `b.grad` після виклику `loss.backward()`. Це може призвести до втрати можливості оновлення параметрів під час тренування моделі. В нашому випадку ми хочемо, щоб тензори `w` та `b` накопичували градієнти, тому вони повинні залишатись листовими.\n",
        "\n",
        "**Висновок:**\n",
        "Ми використовуємо `.data`, щоб виконати операції зміни значень на ваги і зміщення **in-place**, залишаючи їх листовими тензорами, які можуть накопичувати градієнти під час навчання. Це дозволяє коректно працювати механізму зворотного поширення помилки (backpropagation) і оновлювати ваги моделі."
      ],
      "metadata": {
        "id": "nDN1t1RujQsK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Виконайте код та знову обчисліть передбачення, лосс і знайдіть градієнти та виведіть всі ці тензори на екран."
      ],
      "metadata": {
        "id": "rOPSQyttpVjO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.random.manual_seed(1)\n",
        "# w = torch.randn(1, 3, requires_grad=True)  # Листовий тензор\n",
        "w = torch.randn(inputs.shape[1], 1, requires_grad=True)\n",
        "b = torch.randn(1, requires_grad=True)     # Листовий тензор\n",
        "\n",
        "# in-place операції\n",
        "w.data = w.data / 1000\n",
        "b.data = b.data / 1000\n",
        "\n",
        "# Виведення ініційованих значень\n",
        "w, b"
      ],
      "metadata": {
        "id": "-EBOJ3tsnRaD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1780f3d-3d2a-4568-9ded-eb03bcf09674"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[6.6135e-04],\n",
              "         [2.6692e-04],\n",
              "         [6.1677e-05]], requires_grad=True),\n",
              " tensor([0.0006], requires_grad=True))"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Обчислення передбачень з новими вагами та зміщенням\n",
        "predictions = model(inputs_tensor, w, b)\n",
        "\n",
        "# Обчислення втрат\n",
        "loss = binary_cross_entropy(predictions, targets_tensor)\n",
        "\n",
        "# Зворотне поширення помилки\n",
        "loss.backward()\n",
        "\n",
        "# Виведення нових результатів\n",
        "print(\"Нове передбачення:\", predictions)\n",
        "print(\"Нове втрати (Loss):\", loss.item())\n",
        "print(\"Нові градієнти для ваг (w):\", w.grad)\n",
        "print(\"Нові градієнти для зміщення (b):\", b.grad)"
      ],
      "metadata": {
        "id": "-JwXiSpX6orh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0d59939-d319-4a9f-88b8-cf5613e6e69d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Нове передбачення: tensor([[0.5174],\n",
            "        [0.5220],\n",
            "        [0.5244],\n",
            "        [0.5204],\n",
            "        [0.5190]], grad_fn=<MulBackward0>)\n",
            "Нове втрати (Loss): 0.6829456686973572\n",
            "Нові градієнти для ваг (w): tensor([[ -5.4417],\n",
            "        [-18.9853],\n",
            "        [-10.0682]])\n",
            "Нові градієнти для зміщення (b): tensor([-0.0794])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Напишіть алгоритм градієнтного спуску, який буде навчати модель з використанням написаних раніше функцій і виконуючи оновлення ваг. Алгоритм має включати наступні кроки:\n",
        "\n",
        "  1. Генерація прогнозів\n",
        "  2. Обчислення втрат\n",
        "  3. Обчислення градієнтів (gradients) loss-фукнції відносно ваг і зсувів\n",
        "  4. Налаштування ваг шляхом віднімання невеликої величини, пропорційної градієнту (`learning_rate` домножений на градієнт)\n",
        "  5. Скидання градієнтів на нуль\n",
        "\n",
        "Виконайте градієнтний спуск протягом 1000 епох, обчисліть фінальні передбачення і проаналізуйте, чи вони точні?"
      ],
      "metadata": {
        "id": "RCdi44IT334o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Вхідні дані (temp, rainfall, humidity)\n",
        "inputs = np.array([[73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70]], dtype='float32')\n",
        "\n",
        "# Таргети (apples > 80)\n",
        "targets = np.array([[0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1]], dtype='float32')\n",
        "\n",
        "# Перетворення на torch тензори\n",
        "inputs_tensor = torch.from_numpy(inputs)\n",
        "targets_tensor = torch.from_numpy(targets)\n",
        "\n",
        "# Ініціалізація ваг та зсуву\n",
        "torch.random.manual_seed(1)\n",
        "w = torch.randn(inputs.shape[1], 1, requires_grad=True)  # Вектор ваг (3x1)\n",
        "b = torch.randn(1, requires_grad=True)  # Зсув (скаляр)\n",
        "\n",
        "# in-place операції\n",
        "w.data = w.data / 1000\n",
        "b.data = b.data / 1000\n",
        "\n",
        "# Налаштування гіперпараметрів\n",
        "learning_rate = 0.01\n",
        "num_epochs = 1000\n",
        "\n",
        "# Сигмоїдна функція\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + torch.exp(-x))\n",
        "\n",
        "# Функція моделі логістичної регресії\n",
        "def model(x, w, b):\n",
        "    z = torch.matmul(x, w) + b\n",
        "    y_hat = sigmoid(z)\n",
        "    return y_hat\n",
        "\n",
        "# Функція для обчислення втрат з обмеженнями\n",
        "def binary_cross_entropy(predicted_probs, true_labels):\n",
        "    # Обмеження для уникнення log(0)\n",
        "    predicted_probs = torch.clamp(predicted_probs, 1e-7, 1 - 1e-7)\n",
        "    loss = - (true_labels * torch.log(predicted_probs) + (1 - true_labels) * torch.log(1 - predicted_probs))\n",
        "    mean_loss = torch.mean(loss)\n",
        "    return mean_loss\n",
        "\n",
        "# Градієнтний спуск\n",
        "for epoch in range(num_epochs):\n",
        "    # Генерація прогнозів\n",
        "    predictions = model(inputs_tensor, w, b)\n",
        "\n",
        "    # Обчислення втрат\n",
        "    loss = binary_cross_entropy(predictions, targets_tensor)\n",
        "\n",
        "    # Обчислення градієнтів\n",
        "    loss.backward()\n",
        "\n",
        "    # Налаштування ваг\n",
        "    with torch.no_grad():  # Вимкнення відстеження градієнтів для оновлення\n",
        "        w -= learning_rate * w.grad\n",
        "        b -= learning_rate * b.grad\n",
        "\n",
        "    # Скидання градієнтів на нуль\n",
        "    w.grad.zero_()\n",
        "    b.grad.zero_()\n",
        "\n",
        "    # Кожні 100 епох виводимо втрати для моніторингу\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "        print(f'Епоха {epoch + 1}, Втрати: {loss.item()}')\n",
        "\n",
        "# Обчислення фінальних передбачень\n",
        "final_predictions = model(inputs_tensor, w, b)\n",
        "\n",
        "# Аналіз точності передбачень\n",
        "final_predictions_binary = (final_predictions > 0.5).float()  # Перетворення ймовірностей в класи\n",
        "\n",
        "accuracy = (final_predictions_binary.eq(targets_tensor).sum().item()) / targets_tensor.size(0)\n",
        "print(\"Фінальні передбачення:\", final_predictions_binary)\n",
        "print(\"Точність моделі:\", accuracy)\n"
      ],
      "metadata": {
        "id": "mObHPyE06qsO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0eec4383-0e9d-4a90-c94a-e30c085b7cb3"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Епоха 100, Втрати: 6.376954078674316\n",
            "Епоха 200, Втрати: 6.376954078674316\n",
            "Епоха 300, Втрати: 6.376954078674316\n",
            "Епоха 400, Втрати: 6.376954078674316\n",
            "Епоха 500, Втрати: 6.376954078674316\n",
            "Епоха 600, Втрати: 6.376954078674316\n",
            "Епоха 700, Втрати: 6.376954078674316\n",
            "Епоха 800, Втрати: 6.376954078674316\n",
            "Епоха 900, Втрати: 6.376954078674316\n",
            "Епоха 1000, Втрати: 6.376954078674316\n",
            "Фінальні передбачення: tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]])\n",
            "Точність моделі: 0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Цей код працює невірно, так як втрати не оновлюються. Спроба 2."
      ],
      "metadata": {
        "id": "aG7iBhoXU0Fl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Вхідні дані (temp, rainfall, humidity)\n",
        "inputs = np.array([[73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70]], dtype='float32')\n",
        "\n",
        "# Таргети (apples > 80)\n",
        "targets = np.array([[0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1]], dtype='float32')\n",
        "\n",
        "# Перетворення вхідних даних і цілей у тензори\n",
        "inputs = torch.from_numpy(inputs)\n",
        "targets = torch.from_numpy(targets)\n",
        "\n",
        "# Гіперпараметри\n",
        "learning_rate = 1e-5\n",
        "num_epochs = 1000\n",
        "\n",
        "# Визначаємо ваги і зсуви\n",
        "w = torch.randn(1, 3, requires_grad=True)  # Зміна з (2, 3) на (1, 3)\n",
        "b = torch.randn(1, requires_grad=True)      # Зміна з (2) на (1)\n",
        "\n",
        "# Визначаємо модель\n",
        "def model(x, w, b):\n",
        "    return x @ w.t() + b\n",
        "\n",
        "# MSE loss\n",
        "def mse(t1, t2):\n",
        "    diff = t1 - t2\n",
        "    return torch.sum(diff * diff) / diff.numel()\n",
        "\n",
        "# Головна функція тренування\n",
        "def train(inputs, targets, w, b, learning_rate, num_epochs):\n",
        "    for epoch in range(num_epochs):\n",
        "        preds = model(inputs, w, b)\n",
        "        loss = mse(preds, targets)\n",
        "        loss.backward()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            w -= learning_rate * w.grad\n",
        "            b -= learning_rate * b.grad\n",
        "            w.grad.zero_()\n",
        "            b.grad.zero_()\n",
        "\n",
        "        # Виводимо втрати кожні 100 епох\n",
        "        if epoch % 100 == 0:\n",
        "            print(f'Epoch {epoch}: Loss = {loss.item()}')\n",
        "\n",
        "    return preds\n",
        "\n",
        "# Тренування моделі\n",
        "final_preds = train(inputs, targets, w, b, learning_rate, num_epochs)\n",
        "\n",
        "# Виводимо фінальні передбачення та таргети\n",
        "print(\"Final Predictions:\", final_preds)\n",
        "print(\"Targets:\", targets)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYdeTMQPUdzA",
        "outputId": "d3755510-53ea-4dd8-afd7-cc0796913fe8"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Loss = 6522.38720703125\n",
            "Epoch 100: Loss = 47.66027069091797\n",
            "Epoch 200: Loss = 26.315210342407227\n",
            "Epoch 300: Loss = 17.621707916259766\n",
            "Epoch 400: Loss = 12.107441902160645\n",
            "Epoch 500: Loss = 8.349320411682129\n",
            "Epoch 600: Loss = 5.765992164611816\n",
            "Epoch 700: Loss = 3.988471269607544\n",
            "Epoch 800: Loss = 2.7652735710144043\n",
            "Epoch 900: Loss = 1.923520803451538\n",
            "Final Predictions: tensor([[ 0.3812],\n",
            "        [ 1.5041],\n",
            "        [-0.7443],\n",
            "        [-0.5279],\n",
            "        [ 2.7391]], grad_fn=<AddBackward0>)\n",
            "Targets: tensor([[0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Перевірка точності\n",
        "predicted_classes = (final_preds > 0.5).float()\n",
        "accuracy = (predicted_classes == targets).float().mean()\n",
        "print(f'Accuracy: {accuracy.item() * 100:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMlK6a41Ur8I",
        "outputId": "d79f364c-e4c6-4f3a-b420-2f6034f7da2b"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 80.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Секція 2. Створення лог регресії з використанням функціоналу `torch.nn`.**\n",
        "\n",
        "Давайте повторно реалізуємо ту ж модель, використовуючи деякі вбудовані функції та класи з PyTorch.\n",
        "\n",
        "Даних у нас буде побільше - тож, визначаємо нові масиви."
      ],
      "metadata": {
        "id": "fuRhlyF9qAia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "RQ4nNbOPGtBN"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Вхідні дані (temp, rainfall, humidity)\n",
        "inputs = np.array([[73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70],\n",
        "                   [73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70],\n",
        "                   [73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70]], dtype='float32')\n",
        "\n",
        "# Таргети (apples > 80)\n",
        "targets = np.array([[0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1]], dtype='float32')"
      ],
      "metadata": {
        "id": "IX8Bhm74rV4M"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Завантажте вхідні дані та мітки в PyTorch тензори та з них створіть датасет, який поєднує вхідні дані з мітками, використовуючи клас `TensorDataset`. Виведіть перші 3 елементи в датасеті.\n",
        "\n"
      ],
      "metadata": {
        "id": "7X2dV30KtAPu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Перетворення на torch тензори\n",
        "inputs = torch.from_numpy(inputs)\n",
        "targets = torch.from_numpy(targets)\n",
        "\n",
        "# Визначаємо dataset\n",
        "train_ds = TensorDataset(inputs, targets)\n",
        "train_ds[0:3]\n",
        "\n",
        "# Виведення перших 3 елементів у датасеті\n",
        "for i in range(3):\n",
        "    print(f'Елемент {i + 1}: Вхідні дані: {train_ds[i][0]}, Мітка: {train_ds[i][1]}')"
      ],
      "metadata": {
        "id": "chrvMfBs6vjo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d658808d-cbfb-4751-c139-da1203e84056"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Елемент 1: Вхідні дані: tensor([73., 67., 43.]), Мітка: tensor([0.])\n",
            "Елемент 2: Вхідні дані: tensor([91., 88., 64.]), Мітка: tensor([1.])\n",
            "Елемент 3: Вхідні дані: tensor([ 87., 134.,  58.]), Мітка: tensor([1.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Визначте data loader з класом **DataLoader** для підготовленого датасету `train_ds`, встановіть розмір батчу на 5 та увімкніть перемішування даних для ефективного навчання моделі. Виведіть перший елемент в дата лоадері."
      ],
      "metadata": {
        "id": "4nMFaa8suOd3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Визначаємо data loader\n",
        "batch_size = 5\n",
        "train_dl = DataLoader(train_ds, batch_size, shuffle=True)\n",
        "next(iter(train_dl))"
      ],
      "metadata": {
        "id": "ZCsRo5Mx6wEI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f11d1449-c8da-4a32-a4eb-fd54fbd93126"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([[ 73.,  67.,  43.],\n",
              "         [102.,  43.,  37.],\n",
              "         [ 91.,  88.,  64.],\n",
              "         [ 73.,  67.,  43.],\n",
              "         [102.,  43.,  37.]]),\n",
              " tensor([[0.],\n",
              "         [0.],\n",
              "         [1.],\n",
              "         [0.],\n",
              "         [0.]])]"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Створіть клас `LogReg` для логістичної регресії, наслідуючи модуль `torch.nn.Module` за прикладом в лекції (в частині про FeedForward мережі).\n",
        "\n",
        "  У нас модель складається з лінійної комбінації вхідних значень і застосування фукнції сигмоїда. Тож, нейромережа буде складатись з лінійного шару `nn.Linear` і використання активації `nn.Sigmid`. У створеному класі мають бути реалізовані методи `__init__` з ініціалізацією шарів і метод `forward` для виконання прямого проходу моделі через лінійний шар і функцію активації.\n",
        "\n",
        "  Створіть екземпляр класу `LogReg` в змінній `model`."
      ],
      "metadata": {
        "id": "ymcQOo_hum6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LogReg(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(LogReg, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, 1)  # Лінійний шар\n",
        "        self.sigmoid = nn.Sigmoid()  # Сигмоїдальна активація\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear(x)  # Прямий прохід через лінійний шар\n",
        "        x = self.sigmoid(x)  # Застосування сигмоїдальної активації\n",
        "        return x\n",
        "\n",
        "# Визначаємо модель\n",
        "model = nn.Linear(3, 2)\n",
        "print(model.weight)\n",
        "print(model.bias)"
      ],
      "metadata": {
        "id": "EyAwhTBW6xxz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "432e8a74-dd75-4cd2-ab52-17ecc153189d"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.4808,  0.4642,  0.2608],\n",
            "        [ 0.0806, -0.2977,  0.3369]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.5664, -0.0066], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Задайте оптимізатор `Stockastic Gradient Descent` в змінній `opt` для навчання моделі логістичної регресії. А також визначіть в змінній `loss` функцію втрат `binary_cross_entropy` з модуля `torch.nn.functional` для обчислення втрат моделі. Обчисліть втрати для поточних передбачень і міток, а потім виведіть їх. Зробіть висновок, чи моделі вдалось навчитись?"
      ],
      "metadata": {
        "id": "RflV7xeVyoJy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define optimizer\n",
        "opt = torch.optim.SGD(model.parameters(), lr=1e-5)\n",
        "\n",
        "# Визначаємо loss функцію\n",
        "loss_fn = F.mse_loss\n",
        "\n",
        "loss = loss_fn(model(inputs), targets)\n",
        "print(loss)"
      ],
      "metadata": {
        "id": "3QCATPU_6yfa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb29eec6-d2ee-4f44-a97e-9dbefc1443d6"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(4602.0620, grad_fn=<MseLossBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-87-935d2e67c9ea>:7: UserWarning: Using a target size (torch.Size([15, 1])) that is different to the input size (torch.Size([15, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  loss = loss_fn(model(inputs), targets)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Візьміть з лекції функцію для тренування моделі з відстеженням значень втрат і навчіть щойно визначену модель на 1000 епохах. Виведіть після цього графік зміни loss, фінальні передбачення і значення таргетів."
      ],
      "metadata": {
        "id": "ch-WrYnKzMzq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Визначаэмо функцію для навчання моделі\n",
        "def fit(num_epochs, model, loss_fn, opt, train_dl):\n",
        "    for epoch in range(num_epochs):\n",
        "        for xb,yb in train_dl:\n",
        "            # Створення передбачень\n",
        "            pred = model(xb)\n",
        "            loss = loss_fn(pred, yb)\n",
        "\n",
        "            # Виконання градієнтного спуску\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            opt.zero_grad()\n",
        "    print('Training loss: ', loss_fn(model(inputs), targets))"
      ],
      "metadata": {
        "id": "MZwFEgDoMOGL"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Модифікована функцію fit для відстеження втрат\n",
        "def fit_return_loss(num_epochs, model, loss_fn, opt, train_dl):\n",
        "    losses = []\n",
        "    for epoch in range(num_epochs):\n",
        "        # Ініціалізуємо акумулятор для втрат\n",
        "        total_loss = 0\n",
        "\n",
        "        for xb, yb in train_dl:\n",
        "            # Генеруємо передбачення\n",
        "            pred = model(xb)\n",
        "\n",
        "            # Обчислюємо втрати\n",
        "            loss = loss_fn(pred, yb)\n",
        "\n",
        "            # Виконуємо градієнтний спуск\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            opt.zero_grad()\n",
        "\n",
        "            # Накопичуємо втрати\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Обчислюємо середні втрати для епохи\n",
        "        avg_loss = total_loss / len(train_dl)\n",
        "        losses.append(avg_loss)\n",
        "\n",
        "        # Виводимо підсумок епохи\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "          print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
        "    return losses"
      ],
      "metadata": {
        "id": "cEHQH9qE626k"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model for 1000 epochs\n",
        "loss = fit_return_loss(1000, model, loss_fn, opt, train_dl)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDiz2wzMJ-dP",
        "outputId": "53ccee74-bb18-4f2c-b8a3-549c5e4a694e"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-89-4880dda8734d>:13: UserWarning: Using a target size (torch.Size([5, 1])) that is different to the input size (torch.Size([5, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  loss = loss_fn(pred, yb)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/1000], Loss: 21.0467\n",
            "Epoch [20/1000], Loss: 15.4098\n",
            "Epoch [30/1000], Loss: 12.4970\n",
            "Epoch [40/1000], Loss: 10.1592\n",
            "Epoch [50/1000], Loss: 8.1166\n",
            "Epoch [60/1000], Loss: 6.9484\n",
            "Epoch [70/1000], Loss: 6.3935\n",
            "Epoch [80/1000], Loss: 5.7032\n",
            "Epoch [90/1000], Loss: 5.0995\n",
            "Epoch [100/1000], Loss: 4.4380\n",
            "Epoch [110/1000], Loss: 4.1676\n",
            "Epoch [120/1000], Loss: 3.8614\n",
            "Epoch [130/1000], Loss: 3.5972\n",
            "Epoch [140/1000], Loss: 3.4008\n",
            "Epoch [150/1000], Loss: 3.1816\n",
            "Epoch [160/1000], Loss: 3.0234\n",
            "Epoch [170/1000], Loss: 2.8113\n",
            "Epoch [180/1000], Loss: 2.7870\n",
            "Epoch [190/1000], Loss: 2.5188\n",
            "Epoch [200/1000], Loss: 2.3359\n",
            "Epoch [210/1000], Loss: 2.2672\n",
            "Epoch [220/1000], Loss: 2.1703\n",
            "Epoch [230/1000], Loss: 2.0731\n",
            "Epoch [240/1000], Loss: 1.9032\n",
            "Epoch [250/1000], Loss: 1.7538\n",
            "Epoch [260/1000], Loss: 1.7307\n",
            "Epoch [270/1000], Loss: 1.5636\n",
            "Epoch [280/1000], Loss: 1.4682\n",
            "Epoch [290/1000], Loss: 1.4229\n",
            "Epoch [300/1000], Loss: 1.3491\n",
            "Epoch [310/1000], Loss: 1.2999\n",
            "Epoch [320/1000], Loss: 1.2684\n",
            "Epoch [330/1000], Loss: 1.1389\n",
            "Epoch [340/1000], Loss: 1.1154\n",
            "Epoch [350/1000], Loss: 1.0072\n",
            "Epoch [360/1000], Loss: 1.0128\n",
            "Epoch [370/1000], Loss: 0.9211\n",
            "Epoch [380/1000], Loss: 0.8934\n",
            "Epoch [390/1000], Loss: 0.8257\n",
            "Epoch [400/1000], Loss: 0.7893\n",
            "Epoch [410/1000], Loss: 0.7473\n",
            "Epoch [420/1000], Loss: 0.7269\n",
            "Epoch [430/1000], Loss: 0.7236\n",
            "Epoch [440/1000], Loss: 0.6521\n",
            "Epoch [450/1000], Loss: 0.6277\n",
            "Epoch [460/1000], Loss: 0.6037\n",
            "Epoch [470/1000], Loss: 0.5409\n",
            "Epoch [480/1000], Loss: 0.5492\n",
            "Epoch [490/1000], Loss: 0.5088\n",
            "Epoch [500/1000], Loss: 0.4712\n",
            "Epoch [510/1000], Loss: 0.4424\n",
            "Epoch [520/1000], Loss: 0.4423\n",
            "Epoch [530/1000], Loss: 0.4218\n",
            "Epoch [540/1000], Loss: 0.4059\n",
            "Epoch [550/1000], Loss: 0.3742\n",
            "Epoch [560/1000], Loss: 0.3633\n",
            "Epoch [570/1000], Loss: 0.3352\n",
            "Epoch [580/1000], Loss: 0.3308\n",
            "Epoch [590/1000], Loss: 0.3115\n",
            "Epoch [600/1000], Loss: 0.2979\n",
            "Epoch [610/1000], Loss: 0.2965\n",
            "Epoch [620/1000], Loss: 0.2783\n",
            "Epoch [630/1000], Loss: 0.2770\n",
            "Epoch [640/1000], Loss: 0.2482\n",
            "Epoch [650/1000], Loss: 0.2406\n",
            "Epoch [660/1000], Loss: 0.2474\n",
            "Epoch [670/1000], Loss: 0.2214\n",
            "Epoch [680/1000], Loss: 0.2124\n",
            "Epoch [690/1000], Loss: 0.2191\n",
            "Epoch [700/1000], Loss: 0.1976\n",
            "Epoch [710/1000], Loss: 0.1992\n",
            "Epoch [720/1000], Loss: 0.1987\n",
            "Epoch [730/1000], Loss: 0.1826\n",
            "Epoch [740/1000], Loss: 0.1748\n",
            "Epoch [750/1000], Loss: 0.1676\n",
            "Epoch [760/1000], Loss: 0.1659\n",
            "Epoch [770/1000], Loss: 0.1545\n",
            "Epoch [780/1000], Loss: 0.1573\n",
            "Epoch [790/1000], Loss: 0.1474\n",
            "Epoch [800/1000], Loss: 0.1475\n",
            "Epoch [810/1000], Loss: 0.1396\n",
            "Epoch [820/1000], Loss: 0.1392\n",
            "Epoch [830/1000], Loss: 0.1318\n",
            "Epoch [840/1000], Loss: 0.1253\n",
            "Epoch [850/1000], Loss: 0.1281\n",
            "Epoch [860/1000], Loss: 0.1215\n",
            "Epoch [870/1000], Loss: 0.1227\n",
            "Epoch [880/1000], Loss: 0.1171\n",
            "Epoch [890/1000], Loss: 0.1117\n",
            "Epoch [900/1000], Loss: 0.1087\n",
            "Epoch [910/1000], Loss: 0.1137\n",
            "Epoch [920/1000], Loss: 0.1095\n",
            "Epoch [930/1000], Loss: 0.1034\n",
            "Epoch [940/1000], Loss: 0.1007\n",
            "Epoch [950/1000], Loss: 0.1078\n",
            "Epoch [960/1000], Loss: 0.0965\n",
            "Epoch [970/1000], Loss: 0.0962\n",
            "Epoch [980/1000], Loss: 0.0979\n",
            "Epoch [990/1000], Loss: 0.0931\n",
            "Epoch [1000/1000], Loss: 0.0917\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(loss)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "x4xyedf-Obf9",
        "outputId": "9c0675f1-a0a5-4979-b599-1823964cd864"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkQAAAGwCAYAAABIC3rIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA2vElEQVR4nO3de3gU9b3H8c9uQpYE2A0BkyUlIBbLRS5F0LAVKZaUgKlVwXOKpkAryoMNHpEWKUelVI/F0nqtFerxgp5CUfooVVAwBAEv4WJquKnxRg0tbFBpsoCQ2/7OH5CR5SZgkt/gvl/Ps2125rezvxkeyMfvfGfGY4wxAgAAiGNe2xMAAACwjUAEAADiHoEIAADEPQIRAACIewQiAAAQ9whEAAAg7hGIAABA3Eu0PYEzQTQa1Y4dO9SmTRt5PB7b0wEAACfBGKM9e/YoMzNTXu+Ja0AEopOwY8cOZWVl2Z4GAAA4Ddu3b1fHjh1POIZAdBLatGkj6eAB9fv9lmcDAABORiQSUVZWlvN7/EQIRCeh4TSZ3+8nEAEAcIY5mXYXmqoBAEDcIxABAIC4RyACAABxj0AEAADiHoEIAADEPQIRAACIewQiAAAQ9whEAAAg7hGIAABA3CMQAQCAuEcgAgAAcY9ABAAA4h4Pd7WoPmq0s2q/JKlj2xTLswEAIH4RiCz6bF+1Bv32FXk90kez8mxPBwCAuMUpMxcwticAAECcIxBZ5JHH9hQAAIAIRK5gKBEBAGAVgcgiDwUiAABcgUAEAADiHoHIIgpEAAC4A4HIJQyNRAAAWEMgsshDExEAAK5AIHIJCkQAANhDILKI+hAAAO5AIHIJCkQAANhDILKIFiIAANyBQOQSXGUGAIA9BCKLeJYZAADuQCByCepDAADYQyCyiQIRAACuQCByCVqIAACwh0BkEVeZAQDgDgQilzB0EQEAYA2ByCIKRAAAuAOByCXoIQIAwB4CkUU87R4AAHcgEAEAgLhHILKI+hAAAO5gNRDNmTNHffr0kd/vl9/vVygU0ksvveSsP3DggAoKCtSuXTu1bt1ao0aNUkVFRcw2ysvLlZeXp5SUFKWnp2vq1Kmqq6uLGbNq1Sqdf/758vl86tq1q+bNm9ccu3dK6CECAMAeq4GoY8eOuvvuu1VSUqI333xT3/ve93T55Zdr69atkqSbb75ZL7zwghYtWqTVq1drx44dGjlypPP5+vp65eXlqaamRm+88YaefPJJzZs3TzNmzHDGbNu2TXl5ebrkkktUWlqqyZMn67rrrtPy5cubfX+PRAsRAADu4DEue8x6Wlqafve73+mqq67SWWedpQULFuiqq66SJL377rvq0aOHiouLNXDgQL300kv6wQ9+oB07digjI0OSNHfuXE2bNk2ffPKJkpKSNG3aNC1dulRbtmxxvmP06NGqrKzUsmXLTmpOkUhEgUBAVVVV8vv9jbavn9fUqeeMg8Hs7TtylZKU2GjbBgAg3p3K72/X9BDV19dr4cKF2rdvn0KhkEpKSlRbW6ucnBxnTPfu3dWpUycVFxdLkoqLi9W7d28nDElSbm6uIpGIU2UqLi6O2UbDmIZtHEt1dbUikUjMqynwtHsAANzBeiDavHmzWrduLZ/Pp4kTJ+q5555Tz549FQ6HlZSUpNTU1JjxGRkZCofDkqRwOBwThhrWN6w70ZhIJKL9+/cfc06zZs1SIBBwXllZWY2xqyfkrjodAADxxXog6tatm0pLS7Vu3TrdcMMNGjdunN5++22rc5o+fbqqqqqc1/bt25vke+ghAgDAHaw3rSQlJalr166SpP79+2vDhg164IEH9KMf/Ug1NTWqrKyMqRJVVFQoGAxKkoLBoNavXx+zvYar0A4fc+SVaRUVFfL7/UpOTj7mnHw+n3w+X6Ps38miQAQAgD3WK0RHikajqq6uVv/+/dWiRQsVFRU568rKylReXq5QKCRJCoVC2rx5s3bt2uWMKSwslN/vV8+ePZ0xh2+jYUzDNgAAAKxWiKZPn64RI0aoU6dO2rNnjxYsWKBVq1Zp+fLlCgQCGj9+vKZMmaK0tDT5/X7deOONCoVCGjhwoCRp2LBh6tmzp8aMGaPZs2crHA7rtttuU0FBgVPhmThxoh566CHdcsstuvbaa7Vy5Uo988wzWrp0qc1dP4rLLvYDACCuWA1Eu3bt0tixY7Vz504FAgH16dNHy5cv1/e//31J0n333Sev16tRo0apurpaubm5evjhh53PJyQkaMmSJbrhhhsUCoXUqlUrjRs3TnfccYczpkuXLlq6dKluvvlmPfDAA+rYsaMeffRR5ebmNvv+HokeIgAA3MF19yFyo6a6D1F1Xb263XbwXkibZg6Tv2WLRts2AADx7oy8D1E84j5EAAC4A4HIJajTAQBgD4HIInqIAABwBwKRW1AhAgDAGgKRRRSIAABwBwKRSxhKRAAAWEMgsshDExEAAK5AIHIJrjIDAMAeApFF1IcAAHAHApFLUCACAMAeApFFtBABAOAOBCKX4JFyAADYQyCyiKvMAABwBwKRS1AfAgDAHgIRAACIewQil6CFCAAAewhEltFGBACAfQQil+BZZgAA2EMgsowCEQAA9hGI3IICEQAA1hCILONeRAAA2EcgcgkKRAAA2EMgsoz6EAAA9hGIXIL7EAEAYA+ByDJaiAAAsI9A5BLchwgAAHsIRJZ56CICAMA6ApFL0EMEAIA9BCLbKBABAGAdgcglKBABAGAPgcgyCkQAANhHIHIJQxMRAADWEIgs4z5EAADYRyByCQpEAADYQyCyjPsQAQBgH4EIAADEPQKRZfQQAQBgH4HIJeghAgDAHgKRZRSIAACwj0DkEjztHgAAewhElnloIgIAwDqrgWjWrFm64IIL1KZNG6Wnp+uKK65QWVlZzJghQ4bI4/HEvCZOnBgzpry8XHl5eUpJSVF6erqmTp2qurq6mDGrVq3S+eefL5/Pp65du2revHlNvXunhB4iAADssRqIVq9erYKCAq1du1aFhYWqra3VsGHDtG/fvphx119/vXbu3Om8Zs+e7ayrr69XXl6eampq9MYbb+jJJ5/UvHnzNGPGDGfMtm3blJeXp0suuUSlpaWaPHmyrrvuOi1fvrzZ9vV4qA8BAGBfos0vX7ZsWcz7efPmKT09XSUlJRo8eLCzPCUlRcFg8JjbePnll/X2229rxYoVysjI0Le//W3deeedmjZtmmbOnKmkpCTNnTtXXbp00T333CNJ6tGjh1577TXdd999ys3NPWqb1dXVqq6udt5HIpHG2N0TokAEAIA9ruohqqqqkiSlpaXFLJ8/f77at2+vXr16afr06fr888+ddcXFxerdu7cyMjKcZbm5uYpEItq6daszJicnJ2abubm5Ki4uPuY8Zs2apUAg4LyysrIaZf+OiRIRAADWWa0QHS4ajWry5Mm66KKL1KtXL2f5Nddco86dOyszM1ObNm3StGnTVFZWpmeffVaSFA6HY8KQJOd9OBw+4ZhIJKL9+/crOTk5Zt306dM1ZcoU530kEmnaUCSedg8AgE2uCUQFBQXasmWLXnvttZjlEyZMcH7u3bu3OnTooKFDh+rDDz/UN7/5zSaZi8/nk8/na5JtH4kCEQAA9rnilNmkSZO0ZMkSvfLKK+rYseMJx2ZnZ0uSPvjgA0lSMBhURUVFzJiG9w19R8cb4/f7j6oO2UJ9CAAAe6wGImOMJk2apOeee04rV65Uly5dvvQzpaWlkqQOHTpIkkKhkDZv3qxdu3Y5YwoLC+X3+9WzZ09nTFFRUcx2CgsLFQqFGmlPTh/3IQIAwD6rgaigoEB//vOftWDBArVp00bhcFjhcFj79++XJH344Ye68847VVJSon/84x96/vnnNXbsWA0ePFh9+vSRJA0bNkw9e/bUmDFjtHHjRi1fvly33XabCgoKnNNeEydO1EcffaRbbrlF7777rh5++GE988wzuvnmm63t+5FoIQIAwB6rgWjOnDmqqqrSkCFD1KFDB+f19NNPS5KSkpK0YsUKDRs2TN27d9fPf/5zjRo1Si+88IKzjYSEBC1ZskQJCQkKhUL68Y9/rLFjx+qOO+5wxnTp0kVLly5VYWGh+vbtq3vuuUePPvroMS+5b24UiAAAsM9qU/WXXVmVlZWl1atXf+l2OnfurBdffPGEY4YMGaK33nrrlObXvCgRAQBgiyuaquMZBSIAAOwjELkEPUQAANhDILKMq8wAALCPQOQSFIgAALCHQGQZ9SEAAOwjELkEPUQAANhDILKMFiIAAOwjELmEoYsIAABrCETWUSICAMA2ApFL0EMEAIA9BCLL6CECAMA+ApFLUCECAMAeApFlFIgAALCPQOQSXGUGAIA9BCLL6CECAMA+ApFL0EMEAIA9BCLLPHQRAQBgHYEIAADEPQKRZfQQAQBgH4HIJeghAgDAHgKRZRSIAACwj0DkEtyHCAAAewhElnloIgIAwDoCkUvQQwQAgD0EIgAAEPcIRC5BgQgAAHsIRJbRQgQAgH0EIpcwNBEBAGANgcgyKkQAANhHIHIJ6kMAANhDILKMp90DAGAfgcglaCECAMAeApFl9BABAGAfgcg1KBEBAGALgcgyCkQAANhHIHIJeogAALCHQGQZT7sHAMA+ApFLUCACAMAeApFl1IcAALCPQOQS9BABAGCP1UA0a9YsXXDBBWrTpo3S09N1xRVXqKysLGbMgQMHVFBQoHbt2ql169YaNWqUKioqYsaUl5crLy9PKSkpSk9P19SpU1VXVxczZtWqVTr//PPl8/nUtWtXzZs3r6l37+RQIgIAwDqrgWj16tUqKCjQ2rVrVVhYqNraWg0bNkz79u1zxtx888164YUXtGjRIq1evVo7duzQyJEjnfX19fXKy8tTTU2N3njjDT355JOaN2+eZsyY4YzZtm2b8vLydMkll6i0tFSTJ0/Wddddp+XLlzfr/p4IT7sHAMAej3HRb+JPPvlE6enpWr16tQYPHqyqqiqdddZZWrBgga666ipJ0rvvvqsePXqouLhYAwcO1EsvvaQf/OAH2rFjhzIyMiRJc+fO1bRp0/TJJ58oKSlJ06ZN09KlS7Vlyxbnu0aPHq3KykotW7bsS+cViUQUCARUVVUlv9/fqPs89J5V+vCTfXp6wkBln9OuUbcNAEA8O5Xf367qIaqqqpIkpaWlSZJKSkpUW1urnJwcZ0z37t3VqVMnFRcXS5KKi4vVu3dvJwxJUm5uriKRiLZu3eqMOXwbDWMatnGk6upqRSKRmFdTc00qBQAgDrkmEEWjUU2ePFkXXXSRevXqJUkKh8NKSkpSampqzNiMjAyFw2FnzOFhqGF9w7oTjYlEItq/f/9Rc5k1a5YCgYDzysrKapR9PBbuQwQAgH2uCUQFBQXasmWLFi5caHsqmj59uqqqqpzX9u3bm/w73XPiEgCA+JNoewKSNGnSJC1ZskRr1qxRx44dneXBYFA1NTWqrKyMqRJVVFQoGAw6Y9avXx+zvYar0A4fc+SVaRUVFfL7/UpOTj5qPj6fTz6fr1H27ctQHwIAwD6rFSJjjCZNmqTnnntOK1euVJcuXWLW9+/fXy1atFBRUZGzrKysTOXl5QqFQpKkUCikzZs3a9euXc6YwsJC+f1+9ezZ0xlz+DYaxjRsww0MXUQAAFhjtUJUUFCgBQsW6G9/+5vatGnj9PwEAgElJycrEAho/PjxmjJlitLS0uT3+3XjjTcqFApp4MCBkqRhw4apZ8+eGjNmjGbPnq1wOKzbbrtNBQUFTpVn4sSJeuihh3TLLbfo2muv1cqVK/XMM89o6dKl1va9AS1EAADYZ7VCNGfOHFVVVWnIkCHq0KGD83r66aedMffdd59+8IMfaNSoURo8eLCCwaCeffZZZ31CQoKWLFmihIQEhUIh/fjHP9bYsWN1xx13OGO6dOmipUuXqrCwUH379tU999yjRx99VLm5uc26vydEgQgAAGtcdR8it2rK+xDl3rdGZRV7tOC6bH2na/tG3TYAAPHsjL0PUTwjlQIAYA+ByDJ6iAAAsI9A5BKcuAQAwB4CEQAAiHsEIpfgPkQAANhDILKMZ5kBAGAfgcgl6CECAMAeApFl1IcAALCPQOQSFIgAALCHQGQZLUQAANhHIHIJnqACAIA9BCLLqBABAGAfgcglqA8BAGAPgcgyD9eZAQBgHYHILSgRAQBgDYHIMnqIAACwj0DkEjzLDAAAewhEllEgAgDAPgKRS3AbIgAA7DmtQLR9+3b985//dN6vX79ekydP1iOPPNJoE4sbNBEBAGDdaQWia665Rq+88ookKRwO6/vf/77Wr1+vW2+9VXfccUejTjBeUCECAMCe0wpEW7Zs0YUXXihJeuaZZ9SrVy+98cYbmj9/vubNm9eY8/vaoz4EAIB9pxWIamtr5fP5JEkrVqzQD3/4Q0lS9+7dtXPnzsabXRyhQAQAgD2nFYjOO+88zZ07V6+++qoKCws1fPhwSdKOHTvUrl27Rp3g1x0tRAAA2Hdagei3v/2t/vSnP2nIkCG6+uqr1bdvX0nS888/75xKw6nhafcAANiTeDofGjJkiD799FNFIhG1bdvWWT5hwgSlpKQ02uTiAQUiAADsO60K0f79+1VdXe2EoY8//lj333+/ysrKlJ6e3qgTjBfUhwAAsOe0AtHll1+up556SpJUWVmp7Oxs3XPPPbriiis0Z86cRp3g152HJiIAAKw7rUD097//XRdffLEk6a9//asyMjL08ccf66mnntKDDz7YqBOMF7QQAQBgz2kFos8//1xt2rSRJL388ssaOXKkvF6vBg4cqI8//rhRJ/h1R30IAAD7TisQde3aVYsXL9b27du1fPlyDRs2TJK0a9cu+f3+Rp1g/KBEBACALacViGbMmKFf/OIXOvvss3XhhRcqFApJOlgt6tevX6NO8OuOFiIAAOw7rcvur7rqKg0aNEg7d+507kEkSUOHDtWVV17ZaJOLJ/QQAQBgz2kFIkkKBoMKBoPOU+87duzITRlPg4cuIgAArDutU2bRaFR33HGHAoGAOnfurM6dOys1NVV33nmnotFoY88xLlAgAgDAntOqEN1666167LHHdPfdd+uiiy6SJL322muaOXOmDhw4oLvuuqtRJ/m1RoEIAADrTisQPfnkk3r00Uedp9xLUp8+ffSNb3xDP/vZzwhEp4EeIgAA7DmtU2a7d+9W9+7dj1revXt37d69+ytPKp5QIAIAwL7TCkR9+/bVQw89dNTyhx56SH369PnKk4pHhi4iAACsOa1TZrNnz1ZeXp5WrFjh3IOouLhY27dv14svvtioE/y64z5EAADYd1oVou9+97t67733dOWVV6qyslKVlZUaOXKktm7dqv/7v/876e2sWbNGl112mTIzM+XxeLR48eKY9T/5yU/k8XhiXsOHD48Zs3v3buXn58vv9ys1NVXjx4/X3r17Y8Zs2rRJF198sVq2bKmsrCzNnj37dHa7SdFDBACAPad9H6LMzMyjmqc3btyoxx57TI888shJbWPfvn3q27evrr32Wo0cOfKYY4YPH64nnnjCee/z+WLW5+fna+fOnSosLFRtba1++tOfasKECVqwYIEkKRKJaNiwYcrJydHcuXO1efNmXXvttUpNTdWECRNOZZebBPchAgDAvtMORI1hxIgRGjFixAnH+Hw+BYPBY6575513tGzZMm3YsEEDBgyQJP3hD3/QpZdeqt///vfKzMzU/PnzVVNTo8cff1xJSUk677zzVFpaqnvvvdcVgagBBSIAAOw5rVNmzWnVqlVKT09Xt27ddMMNN+izzz5z1hUXFys1NdUJQ5KUk5Mjr9erdevWOWMGDx6spKQkZ0xubq7Kysr073//+5jfWV1drUgkEvNqKvQQAQBgn6sD0fDhw/XUU0+pqKhIv/3tb7V69WqNGDFC9fX1kqRwOKz09PSYzyQmJiotLU3hcNgZk5GRETOm4X3DmCPNmjVLgUDAeWVlZTX2rh3F0EQEAIA1p3TK7Hh9Pg0qKyu/ylyOMnr0aOfn3r17q0+fPvrmN7+pVatWaejQoY36XYebPn26pkyZ4ryPRCJNFoqoEAEAYN8pBaJAIPCl68eOHfuVJnQi55xzjtq3b68PPvhAQ4cOVTAY1K5du2LG1NXVaffu3U7fUTAYVEVFRcyYhvfH603y+XxHNW8DAICvr1MKRIdf7WXDP//5T3322Wfq0KGDJCkUCqmyslIlJSXq37+/JGnlypWKRqPKzs52xtx6662qra1VixYtJEmFhYXq1q2b2rZta2dHDsNVZgAA2Ge1h2jv3r0qLS1VaWmpJGnbtm0qLS1VeXm59u7dq6lTp2rt2rX6xz/+oaKiIl1++eXq2rWrcnNzJUk9evTQ8OHDdf3112v9+vV6/fXXNWnSJI0ePVqZmZmSpGuuuUZJSUkaP368tm7dqqeffloPPPBAzCkxN6CFCAAAe6wGojfffFP9+vVTv379JElTpkxRv379NGPGDCUkJGjTpk364Q9/qG9961saP368+vfvr1dffTXmdNb8+fPVvXt3DR06VJdeeqkGDRoUcx+kQCCgl19+Wdu2bVP//v3185//XDNmzHDNJff0EAEAYJ/V+xANGTLkhFdXLV++/Eu3kZaW5tyE8Xj69OmjV1999ZTn15x4lhkAAPa4+rJ7AACA5kAgcgl6iAAAsIdAZJmHJiIAAKwjELkEFSIAAOwhEFlGfQgAAPsIRC5BgQgAAHsIRJbRQgQAgH0EIpfgafcAANhDILKMAhEAAPYRiFyC+hAAAPYQiCzjPkQAANhHIHILSkQAAFhDILKM+hAAAPYRiFyCp90DAGAPgcgyWogAALCPQOQS3IYIAAB7CETWUSICAMA2ApFLUCACAMAeApFl9BABAGAfgcgl6CECAMAeApFlFIgAALCPQOQS3IcIAAB7CESW0UMEAIB9BCKXoIcIAAB7CESWeegiAgDAOgKRS1AgAgDAHgKRZfQQAQBgH4HILWgiAgDAGgKRZVSIAACwj0DkEtSHAACwh0BkGVeZAQBgH4HIJWghAgDAHgKRbRSIAACwjkDkEoYSEQAA1hCILKNABACAfQQil6A+BACAPQQiAAAQ9whElnkO3ZmRFiIAAOwhEAEAgLhHILKsoamaAhEAAPYQiAAAQNyzGojWrFmjyy67TJmZmfJ4PFq8eHHMemOMZsyYoQ4dOig5OVk5OTl6//33Y8bs3r1b+fn58vv9Sk1N1fjx47V3796YMZs2bdLFF1+sli1bKisrS7Nnz27qXTtpDQ935T5EAADYYzUQ7du3T3379tUf//jHY66fPXu2HnzwQc2dO1fr1q1Tq1atlJubqwMHDjhj8vPztXXrVhUWFmrJkiVas2aNJkyY4KyPRCIaNmyYOnfurJKSEv3ud7/TzJkz9cgjjzT5/p0M7kMEAIB9iTa/fMSIERoxYsQx1xljdP/99+u2227T5ZdfLkl66qmnlJGRocWLF2v06NF65513tGzZMm3YsEEDBgyQJP3hD3/QpZdeqt///vfKzMzU/PnzVVNTo8cff1xJSUk677zzVFpaqnvvvTcmOB2uurpa1dXVzvtIJNLIe/4FL1eZAQBgnWt7iLZt26ZwOKycnBxnWSAQUHZ2toqLiyVJxcXFSk1NdcKQJOXk5Mjr9WrdunXOmMGDByspKckZk5ubq7KyMv373/8+5nfPmjVLgUDAeWVlZTXFLh50qEQUJREBAGCNawNROByWJGVkZMQsz8jIcNaFw2Glp6fHrE9MTFRaWlrMmGNt4/DvONL06dNVVVXlvLZv3/7Vd+g4nApRk30DAAD4MlZPmbmVz+eTz+drlu/yUiECAMA611aIgsGgJKmioiJmeUVFhbMuGAxq165dMevr6uq0e/fumDHH2sbh32GTR/QQAQBgm2sDUZcuXRQMBlVUVOQsi0QiWrdunUKhkCQpFAqpsrJSJSUlzpiVK1cqGo0qOzvbGbNmzRrV1tY6YwoLC9WtWze1bdu2mfbm+LyH/gS47B4AAHusBqK9e/eqtLRUpaWlkg42UpeWlqq8vFwej0eTJ0/W//zP/+j555/X5s2bNXbsWGVmZuqKK66QJPXo0UPDhw/X9ddfr/Xr1+v111/XpEmTNHr0aGVmZkqSrrnmGiUlJWn8+PHaunWrnn76aT3wwAOaMmWKpb0+0sEKUZQ8BACANVZ7iN58801dcsklzvuGkDJu3DjNmzdPt9xyi/bt26cJEyaosrJSgwYN0rJly9SyZUvnM/Pnz9ekSZM0dOhQeb1ejRo1Sg8++KCzPhAI6OWXX1ZBQYH69++v9u3ba8aMGce95L65eZ0bM9qdBwAA8cxjOFfzpSKRiAKBgKqqquT3+xt127cv3qL/W/uxbhp6rm7+/rcaddsAAMSzU/n97doeonjBozsAALCPQGQZ9yECAMA+ApFLcB8iAADsIRBZxrPMAACwj0Bk2Rd3qrY7DwAA4hmByDKaqgEAsI9AZBlN1QAA2EcgssxzKBBFOWcGAIA1BCLLPPQQAQBgHYHIMufRHZw0AwDAGgKRZR5x2T0AALYRiCzzcpUZAADWEYgsc5qqyUMAAFhDILLsi6ZqEhEAALYQiCzjPkQAANhHILLsUIGIHiIAACwiEFnm9XKVGQAAthGILKOHCAAA+whEljXch4irzAAAsIdAZNkX9yGyOw8AAOIZgcgyDzdmBADAOgKRZV7nxowEIgAAbCEQWebhPkQAAFhHILKs4T5ENFUDAGAPgcgyHu4KAIB9BCLLuDEjAAD2EYgs++KUGYkIAABbCESWOU3V5CEAAKwhEFnGozsAALCPQGSZl8vuAQCwjkBkGVeZAQBgH4HIMh7uCgCAfQQiy3iWGQAA9hGILPN4qBABAGAbgcgyp4fI7jQAAIhrBCLLnKvMOGUGAIA1BCLLuA8RAAD2EYgs407VAADYRyCyzEuFCAAA6whElnEfIgAA7HN1IJo5c6Y8Hk/Mq3v37s76AwcOqKCgQO3atVPr1q01atQoVVRUxGyjvLxceXl5SklJUXp6uqZOnaq6urrm3pXjaqgQcZkZAAD2JNqewJc577zztGLFCud9YuIXU7755pu1dOlSLVq0SIFAQJMmTdLIkSP1+uuvS5Lq6+uVl5enYDCoN954Qzt37tTYsWPVokUL/eY3v2n2fTkWmqoBALDP9YEoMTFRwWDwqOVVVVV67LHHtGDBAn3ve9+TJD3xxBPq0aOH1q5dq4EDB+rll1/W22+/rRUrVigjI0Pf/va3deedd2ratGmaOXOmkpKSmnt3juLh4a4AAFjn6lNmkvT+++8rMzNT55xzjvLz81VeXi5JKikpUW1trXJycpyx3bt3V6dOnVRcXCxJKi4uVu/evZWRkeGMyc3NVSQS0datW4/7ndXV1YpEIjGvpuJ17lRNJAIAwBZXB6Ls7GzNmzdPy5Yt05w5c7Rt2zZdfPHF2rNnj8LhsJKSkpSamhrzmYyMDIXDYUlSOByOCUMN6xvWHc+sWbMUCAScV1ZWVuPu2GEaWohoqgYAwB5XnzIbMWKE83OfPn2UnZ2tzp0765lnnlFycnKTfe/06dM1ZcoU530kEmmyUORtiKRUiAAAsMbVFaIjpaam6lvf+pY++OADBYNB1dTUqLKyMmZMRUWF03MUDAaPuuqs4f2x+pIa+Hw++f3+mFdT4bJ7AADsO6MC0d69e/Xhhx+qQ4cO6t+/v1q0aKGioiJnfVlZmcrLyxUKhSRJoVBImzdv1q5du5wxhYWF8vv96tmzZ7PP/1g8zsNdSUQAANji6lNmv/jFL3TZZZepc+fO2rFjh371q18pISFBV199tQKBgMaPH68pU6YoLS1Nfr9fN954o0KhkAYOHChJGjZsmHr27KkxY8Zo9uzZCofDuu2221RQUCCfz2d57w5ymqqjlicCAEAcc3Ug+uc//6mrr75an332mc466ywNGjRIa9eu1VlnnSVJuu++++T1ejVq1ChVV1crNzdXDz/8sPP5hIQELVmyRDfccINCoZBatWqlcePG6Y477rC1S0fhPkQAANjnMYbfxF8mEokoEAioqqqq0fuJXv/gU+U/uk7dg220bPLgRt02AADx7FR+f59RPURfR19cdk8uBQDAFgKRZR4PV5kBAGAbgciyhoe7cuYSAAB7CESWOc8yIw8BAGANgcgyp0JkdxoAAMQ1ApFlHh7uCgCAdQQiy7gPEQAA9hGILPPSQwQAgHUEIssa7kNEIAIAwB4CkWVfVIhIRAAA2EIgsqyhh6ieQAQAgDUEIsuSEg/+EdTVE4gAALCFQGRZ4qEbEdXURy3PBACA+EUgsqxFAhUiAABsIxBZ1hCIaqkQAQBgDYHIssSEg6fM6qKGK80AALCEQGRZQ4VIOhiKAABA8yMQWdYiweP8zGkzAADsIBBZluj94o+glsZqAACsIBBZRoUIAAD7CESWeTwe515EXHoPAIAdBCIXaLjSjAoRAAB2EIhcgHsRAQBgF4HIBZy7VXPZPQAAVhCIXKChsbqmjgoRAAA2EIhcoOHSeypEAADYQSBygYYKUR09RAAAWEEgcoGGHqIaAhEAAFYQiFwgsaGpmvsQAQBgBYHIBVpwHyIAAKwiELnAF/chokIEAIANBCIXaHh0BxUiAADsIBC5QGtfoiRpX3Wd5ZkAABCfCEQuEEhuIUmq2l9reSYAAMQnApEL+AlEAABYRSByAQIRAAB2EYhcgFNmAADYRSBygYZAFDlAUzUAADYQiFwg9VAg+mxvteWZAAAQnwhELvCtjDaSpPcq9uhAbb3l2QAAEH/iKhD98Y9/1Nlnn62WLVsqOztb69evtz0lSVJWWrLat/aptt7ooZUf6O0dEe5JBABAM/IYY+LieRFPP/20xo4dq7lz5yo7O1v333+/Fi1apLKyMqWnp5/ws5FIRIFAQFVVVfL7/U0yv3tfLtODKz+IWZbWKklprZLUKS1Faa2S1K5Vklr7EuVr4VVrXwt5PFIrX6LObpeilKQEtfIlqmVigpKTEuRL9Mrj8TTJXAEAOBOcyu/vuAlE2dnZuuCCC/TQQw9JkqLRqLKysnTjjTfql7/85Qk/2xyBqD5q9MTr2/TcW//SzqoD2r2v5itvMynRq6QErxK8HrVI8CjB61Gi16vEBI8SvR61OLQuMcGrRG/sMmd8glctvB4leL1K8EoeeeTxSAez1sHPJCclHHxvpKgxMkbyej1qeSiUeTwHP+c99LmGoHbU8kPbPrju0HIdOf7gD97DxnskeZ2VX/BISvB6Dq6TdHg+dLbpvD+4vcO/p2EfnTkd+pznsG19se7wjcf83zG/6/DPeI7+6GHb+GLlscYdHnqPvV5HiF1wzO8+xpy/bN4ncqxxnhPM41Q+92Xjv/jcqX/m+J86ve86FY3xHzONM49G2EYjzIT/tosPCV6PMlOTG3Wbp/L7O7FRv9mlampqVFJSounTpzvLvF6vcnJyVFxcfNT46upqVVd/0eAciUSafI4JXo+uu/gcXXfxOZKkqs9rtTOyXzurDqii6oB2f16j3XtrtLe6Tvtr67Wvul6SUThyQJ/trdHnNfXaV12nuugX+bamLqqaOp6PBgBwv/Q2Pq2/Ncfa98dFIPr0009VX1+vjIyMmOUZGRl69913jxo/a9Ys/frXv26u6R1TIKWFAikt1D14ahWpuvqoDtRFdaC2Xgdq61VXb1QXjaq23qg+alRbHz30/4feR6OqqzeqP2JMXdQcfB02PnqomGgOVYGMpLqo0f6aOqcqdKioImOkA7X1TsXoYE479DkjGRlFD/vZqS5JzrYbvqeBOc7nG7YbNSbmv6yj5uD+SHK207CN4y479D/OdzV89xFzO3wDRkdvL+b94ds+tK3DHfdzMWO+eGeO+uHYY48s/R5ZCzaHjYg5zsepGR9zDqfwHccf8+XbOfG3nmDOx/3E0X8OJ/+509vmyWqUkn0jbKQx5uGa49HEzoTzLMf6u+g2vhZ225rjIhCdqunTp2vKlCnO+0gkoqysLIszOnmJCV61TvA6D4wFAABfLi5+a7Zv314JCQmqqKiIWV5RUaFgMHjUeJ/PJ5/P11zTAwAAlsXFZfdJSUnq37+/ioqKnGXRaFRFRUUKhUIWZwYAANwgLipEkjRlyhSNGzdOAwYM0IUXXqj7779f+/bt009/+lPbUwMAAJbFTSD60Y9+pE8++UQzZsxQOBzWt7/9bS1btuyoRmsAABB/4uY+RF9Fc9yHCAAANK5T+f0dFz1EAAAAJ0IgAgAAcY9ABAAA4h6BCAAAxD0CEQAAiHsEIgAAEPcIRAAAIO4RiAAAQNwjEAEAgLgXN4/u+CoabuYdiUQszwQAAJysht/bJ/NQDgLRSdizZ48kKSsry/JMAADAqdqzZ48CgcAJx/Ass5MQjUa1Y8cOtWnTRh6Pp1G3HYlElJWVpe3bt/OctCbEcW4eHOfmw7FuHhzn5tFUx9kYoz179igzM1Ne74m7hKgQnQSv16uOHTs26Xf4/X7+sjUDjnPz4Dg3H4518+A4N4+mOM5fVhlqQFM1AACIewQiAAAQ9whElvl8Pv3qV7+Sz+ezPZWvNY5z8+A4Nx+OdfPgODcPNxxnmqoBAEDco0IEAADiHoEIAADEPQIRAACIewQiAAAQ9whEFv3xj3/U2WefrZYtWyo7O1vr16+3PaUzyqxZs3TBBReoTZs2Sk9P1xVXXKGysrKYMQcOHFBBQYHatWun1q1ba9SoUaqoqIgZU15erry8PKWkpCg9PV1Tp05VXV1dc+7KGeXuu++Wx+PR5MmTnWUc58bzr3/9Sz/+8Y/Vrl07JScnq3fv3nrzzTed9cYYzZgxQx06dFBycrJycnL0/vvvx2xj9+7dys/Pl9/vV2pqqsaPH6+9e/c29664Vn19vW6//XZ16dJFycnJ+uY3v6k777wz5nlXHOdTt2bNGl122WXKzMyUx+PR4sWLY9Y31jHdtGmTLr74YrVs2VJZWVmaPXt24+yAgRULFy40SUlJ5vHHHzdbt241119/vUlNTTUVFRW2p3bGyM3NNU888YTZsmWLKS0tNZdeeqnp1KmT2bt3rzNm4sSJJisryxQVFZk333zTDBw40HznO99x1tfV1ZlevXqZnJwc89Zbb5kXX3zRtG/f3kyfPt3GLrne+vXrzdlnn2369OljbrrpJmc5x7lx7N6923Tu3Nn85Cc/MevWrTMfffSRWb58ufnggw+cMXfffbcJBAJm8eLFZuPGjeaHP/yh6dKli9m/f78zZvjw4aZv375m7dq15tVXXzVdu3Y1V199tY1dcqW77rrLtGvXzixZssRs27bNLFq0yLRu3do88MADzhiO86l78cUXza233mqeffZZI8k899xzMesb45hWVVWZjIwMk5+fb7Zs2WL+8pe/mOTkZPOnP/3pK8+fQGTJhRdeaAoKCpz39fX1JjMz08yaNcvirM5su3btMpLM6tWrjTHGVFZWmhYtWphFixY5Y9555x0jyRQXFxtjDv4F9nq9JhwOO2PmzJlj/H6/qa6ubt4dcLk9e/aYc8891xQWFprvfve7TiDiODeeadOmmUGDBh13fTQaNcFg0Pzud79zllVWVhqfz2f+8pe/GGOMefvtt40ks2HDBmfMSy+9ZDwej/nXv/7VdJM/g+Tl5Zlrr702ZtnIkSNNfn6+MYbj3BiODESNdUwffvhh07Zt25h/N6ZNm2a6dev2lefMKTMLampqVFJSopycHGeZ1+tVTk6OiouLLc7szFZVVSVJSktLkySVlJSotrY25jh3795dnTp1co5zcXGxevfurYyMDGdMbm6uIpGItm7d2oyzd7+CggLl5eXFHE+J49yYnn/+eQ0YMED/8R//ofT0dPXr10//+7//66zftm2bwuFwzLEOBALKzs6OOdapqakaMGCAMyYnJ0der1fr1q1rvp1xse985zsqKirSe++9J0nauHGjXnvtNY0YMUISx7kpNNYxLS4u1uDBg5WUlOSMyc3NVVlZmf79739/pTnycFcLPv30U9XX18f8cpCkjIwMvfvuu5ZmdWaLRqOaPHmyLrroIvXq1UuSFA6HlZSUpNTU1JixGRkZCofDzphj/Tk0rMNBCxcu1N///ndt2LDhqHUc58bz0Ucfac6cOZoyZYr++7//Wxs2bNB//dd/KSkpSePGjXOO1bGO5eHHOj09PWZ9YmKi0tLSONaH/PKXv1QkElH37t2VkJCg+vp63XXXXcrPz5ckjnMTaKxjGg6H1aVLl6O20bCubdu2pz1HAhG+FgoKCrRlyxa99tprtqfytbN9+3bddNNNKiwsVMuWLW1P52stGo1qwIAB+s1vfiNJ6tevn7Zs2aK5c+dq3Lhxlmf39fHMM89o/vz5WrBggc477zyVlpZq8uTJyszM5DjHMU6ZWdC+fXslJCQcdRVORUWFgsGgpVmduSZNmqQlS5bolVdeUceOHZ3lwWBQNTU1qqysjBl/+HEOBoPH/HNoWIeDp8R27dql888/X4mJiUpMTNTq1av14IMPKjExURkZGRznRtKhQwf17NkzZlmPHj1UXl4u6YtjdaJ/O4LBoHbt2hWzvq6uTrt37+ZYHzJ16lT98pe/1OjRo9W7d2+NGTNGN998s2bNmiWJ49wUGuuYNuW/JQQiC5KSktS/f38VFRU5y6LRqIqKihQKhSzO7MxijNGkSZP03HPPaeXKlUeVUfv3768WLVrEHOeysjKVl5c7xzkUCmnz5s0xfwkLCwvl9/uP+sUUr4YOHarNmzertLTUeQ0YMED5+fnOzxznxnHRRRcddeuI9957T507d5YkdenSRcFgMOZYRyIRrVu3LuZYV1ZWqqSkxBmzcuVKRaNRZWdnN8NeuN/nn38urzf2119CQoKi0agkjnNTaKxjGgqFtGbNGtXW1jpjCgsL1a1bt690ukwSl93bsnDhQuPz+cy8efPM22+/bSZMmGBSU1NjrsLBid1www0mEAiYVatWmZ07dzqvzz//3BkzceJE06lTJ7Ny5Urz5ptvmlAoZEKhkLO+4XLwYcOGmdLSUrNs2TJz1llncTn4lzj8KjNjOM6NZf369SYxMdHcdddd5v333zfz5883KSkp5s9//rMz5u677zapqanmb3/7m9m0aZO5/PLLj3npcr9+/cy6devMa6+9Zs4999y4vhz8SOPGjTPf+MY3nMvun332WdO+fXtzyy23OGM4zqduz5495q233jJvvfWWkWTuvfde89Zbb5mPP/7YGNM4x7SystJkZGSYMWPGmC1btpiFCxealJQULrs/0/3hD38wnTp1MklJSebCCy80a9eutT2lM4qkY76eeOIJZ8z+/fvNz372M9O2bVuTkpJirrzySrNz586Y7fzjH/8wI0aMMMnJyaZ9+/bm5z//uamtrW3mvTmzHBmIOM6N54UXXjC9evUyPp/PdO/e3TzyyCMx66PRqLn99ttNRkaG8fl8ZujQoaasrCxmzGeffWauvvpq07p1a+P3+81Pf/pTs2fPnubcDVeLRCLmpptuMp06dTItW7Y055xzjrn11ltjLuXmOJ+6V1555Zj/Jo8bN84Y03jHdOPGjWbQoEHG5/OZb3zjG+buu+9ulPl7jDns1pwAAABxiB4iAAAQ9whEAAAg7hGIAABA3CMQAQCAuEcgAgAAcY9ABAAA4h6BCAAAxD0CEQAAiHsEIgA4TR6PR4sXL7Y9DQCNgEAE4Iz0k5/8RB6P56jX8OHDbU8NwBko0fYEAOB0DR8+XE888UTMMp/PZ2k2AM5kVIgAnLF8Pp+CwWDMq23btpIOns6aM2eORowYoeTkZJ1zzjn661//GvP5zZs363vf+56Sk5PVrl07TZgwQXv37o0Z8/jjj+u8886Tz+dThw4dNGnSpJj1n376qa688kqlpKTo3HPP1fPPP9+0Ow2gSRCIAHxt3X777Ro1apQ2btyo/Px8jR49Wu+8844kad++fcrNzVXbtm21YcMGLVq0SCtWrIgJPHPmzFFBQYEmTJigzZs36/nnn1fXrl1jvuPXv/61/vM//1ObNm3SpZdeqvz8fO3evbtZ9xNAIzAAcAYaN26cSUhIMK1atYp53XXXXcYYYySZiRMnxnwmOzvb3HDDDcYYYx555BHTtm1bs3fvXmf90qVLjdfrNeFw2BhjTGZmprn11luPOwdJ5rbbbnPe792710gyL730UqPtJ4DmQQ8RgDPWJZdcojlz5sQsS0tLc34OhUIx60KhkEpLSyVJ77zzjvr27atWrVo56y+66CJFo1GVlZXJ4/Fox44dGjp06Ann0KdPH+fnVq1aye/3a9euXae7SwAsIRABOGO1atXqqFNYjSU5OfmkxrVo0SLmvcfjUTQabYopAWhC9BAB+Npau3btUe979OghSerRo4c2btyoffv2Oetff/11eb1edevWTW3atNHZZ5+toqKiZp0zADuoEAE4Y1VXVyscDscsS0xMVPv27SVJixYt0oABAzRo0CDNnz9f69ev12OPPSZJys/P169+9SuNGzdOM2fO1CeffKIbb7xRY8aMUUZGhiRp5syZmjhxotLT0zVixAjt2bNHr7/+um688cbm3VEATY5ABOCMtWzZMnXo0CFmWbdu3fTuu+9KOngF2MKFC/Wzn/1MHTp00F/+8hf17NlTkpSSkqLly5frpptu0gUXXKCUlBSNGjVK9957r7OtcePG6cCBA7rvvvv0i1/8Qu3bt9dVV13VfDsIoNl4jDHG9iQAoLF5PB4999xzuuKKK2xPBcAZgB4iAAAQ9whEAAAg7tFDBOBriW4AAKeCChEAAIh7BCIAABD3CEQAACDuEYgAAEDcIxABAIC4RyACAABxj0AEAADiHoEIAADEvf8H3pyHgqUeT5AAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate predictions\n",
        "preds = model(inputs)\n",
        "preds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hqb3HjsuOevH",
        "outputId": "e02ee521-011e-429e-8789-544331285550"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.5258,  0.4176],\n",
              "        [ 0.6520,  0.8902],\n",
              "        [ 0.9489,  0.6258],\n",
              "        [-0.0583, -0.1384],\n",
              "        [ 1.0416,  1.3869],\n",
              "        [ 0.5258,  0.4176],\n",
              "        [ 0.6520,  0.8902],\n",
              "        [ 0.9489,  0.6258],\n",
              "        [-0.0583, -0.1384],\n",
              "        [ 1.0416,  1.3869],\n",
              "        [ 0.5258,  0.4176],\n",
              "        [ 0.6520,  0.8902],\n",
              "        [ 0.9489,  0.6258],\n",
              "        [-0.0583, -0.1384],\n",
              "        [ 1.0416,  1.3869]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds.int()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSnFJr6_OhIj",
        "outputId": "78a91611-8b59-4b0a-8a82-c41b05c23f8c"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0, 0],\n",
              "        [0, 0],\n",
              "        [1, 0],\n",
              "        [0, 0],\n",
              "        [0, 1],\n",
              "        [0, 0],\n",
              "        [0, 0],\n",
              "        [1, 0],\n",
              "        [0, 0],\n",
              "        [0, 1],\n",
              "        [0, 0],\n",
              "        [0, 0],\n",
              "        [1, 0],\n",
              "        [0, 0],\n",
              "        [0, 1]], dtype=torch.int32)"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare with targets\n",
        "targets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWITqiklOj4r",
        "outputId": "68d65e14-59e1-407c-f377-7a13b9686a63"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    }
  ]
}